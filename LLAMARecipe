{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":1349767,"sourceType":"datasetVersion","datasetId":785504}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport transformers\nfrom transformers import T5Tokenizer, T5ForConditionalGeneration\nimport torch\nfrom datasets import load_dataset\nimport wandb\nimport numpy as np\nimport math\nimport random","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-04T17:02:00.480137Z","iopub.execute_input":"2025-04-04T17:02:00.480433Z","iopub.status.idle":"2025-04-04T17:02:25.228341Z","shell.execute_reply.started":"2025-04-04T17:02:00.480395Z","shell.execute_reply":"2025-04-04T17:02:25.227729Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"# import dataset\n# dataset obtained from \n# https://www.kaggle.com/datasets/snehallokesh31096/recipe\ndf = pd.read_csv('/kaggle/input/recipe/recipes_82k.csv')\n# dropping unnecessary columns\ndf.drop(['category'],axis=1,inplace=True)\n# concatenating the title and recipe\ndf['recipe']=df.apply(lambda x:'%s: %s' % (df['recipe_name'],df['cooking_method']),axis=1)\n# saving df as a CSV file\ndf.to_csv('/kaggle/working/recipe_cleaned.csv', index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T17:02:25.229150Z","iopub.execute_input":"2025-04-04T17:02:25.229521Z","iopub.status.idle":"2025-04-04T17:04:08.646596Z","shell.execute_reply.started":"2025-04-04T17:02:25.229483Z","shell.execute_reply":"2025-04-04T17:04:08.645831Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"# TODO: create a randomly generated list for train and test\n# by using random numbers within a given range and store in an array\nstart = 0\nend = len(df)-1\nindx = np.array([random.randint(start, end) for i in range(math.floor(len(df)*.80))])\n\n# loading df for train and test\ntrain_data = df.iloc[indx]\ntest_data = df.iloc[-indx]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T17:04:08.647493Z","iopub.execute_input":"2025-04-04T17:04:08.647757Z","iopub.status.idle":"2025-04-04T17:04:08.762418Z","shell.execute_reply.started":"2025-04-04T17:04:08.647734Z","shell.execute_reply":"2025-04-04T17:04:08.761540Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# COPIED FROM https://www.kaggle.com/code/kreeshrajani/fine-tune-t5-for-conversational-model\nclass T5Dataset:\n  def __init__(self, inps, outs, tokenizer, inp_max_len, out_max_len):   \n    self.inps = inps\n    self.outs = outs\n    self.tokenizer = tokenizer\n    self.input_max_len = inp_max_len\n    self.output_max_len = out_max_len\n  \n  def __len__(self):                      # This method retrives the number of item from the dataset\n    return len(self.inps)\n\n  def __getitem__(self, item):             # This method retrieves the item at the specified index item. \n    inp = str(self.inps[item])\n    out = str(self.outs[item])\n\n    input_tokenize = self.tokenizer(      \n            inp,\n            add_special_tokens=True,\n            max_length=self.input_max_len,\n            padding = 'max_length',\n            truncation = True,\n            return_attention_mask=True,\n            return_tensors=\"pt\"\n        )\n    output_tokenize = self.tokenizer(\n            out,\n            add_special_tokens=True,\n            max_length=self.output_max_len,\n            padding = 'max_length',\n            truncation = True,\n            return_attention_mask=True,\n            return_tensors=\"pt\"\n            \n        )\n    \n\n    input_ids = input_tokenize[\"input_ids\"].flatten().to(dtype=torch.long)\n    attention_mask = input_tokenize[\"attention_mask\"].flatten().to(dtype=torch.long)\n    output_ids = output_tokenize['input_ids'].flatten().to(dtype=torch.long)\n\n    out = {\n            'input': inp,      \n            'target': out,\n            'input_ids': input_ids,\n            'attention_mask': attention_mask,\n            'target_ids': output_ids\n        }\n        \n    return out ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T17:04:08.765115Z","iopub.execute_input":"2025-04-04T17:04:08.765394Z","iopub.status.idle":"2025-04-04T17:04:08.772589Z","shell.execute_reply.started":"2025-04-04T17:04:08.765372Z","shell.execute_reply":"2025-04-04T17:04:08.771528Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"def train(tokenizer, model, device, loader, optimizer, fp16=True):\n    losses = []\n    if fp16: model.half()\n    model.train()\n    for _, data in enumerate(loader, 0):\n        y = train_data['recipe'].to(device, dtype = torch.long)\n        y_ids = y[:, :-1].contiguous()\n        lm_labels = y[:, 1:].clone().detach()\n        lm_labels[y[:, 1:] == tokenizer.pad_token_id] = -100\n        ids = data['input_ids'].to(device, dtype = torch.long)\n        mask = data['attention_mask'].to(device, dtype = torch.long)\n\n        outputs = model(input_ids = ids, attention_mask = mask, decoder_input_ids=y_ids, labels=lm_labels)\n        loss = outputs[0]\n        losses.append(loss.item())\n        \n        if _%10 == 0:\n            wandb.log({\"Training Loss\": loss.item()})\n        \n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n    return losses     ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T17:07:05.610790Z","iopub.execute_input":"2025-04-04T17:07:05.611173Z","iopub.status.idle":"2025-04-04T17:07:05.619099Z","shell.execute_reply.started":"2025-04-04T17:07:05.611144Z","shell.execute_reply":"2025-04-04T17:07:05.618098Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"# test dataset\ndef test(tokenizer, model, device, loader, fp16=True):\n    losses = []\n    if fp16: model.half()\n    model.eval()\n    with torch.no_grad():\n        for _, data in enumerate(loader, 0):\n            y = test_data['recipe'].to(device, dtype = torch.long)\n            y_ids = y[:, :-1].contiguous()\n            lm_labels = y[:, 1:].clone().detach()\n            lm_labels[y[:, 1:] == tokenizer.pad_token_id] = -100\n            ids = data['input_ids'].to(device, dtype = torch.long)\n            mask = data['attention_mask'].to(device, dtype = torch.long)\n\n            outputs = model(input_ids = ids, attention_mask = mask, decoder_input_ids=y_ids, labels=lm_labels)\n            loss = outputs[0]\n            losses.append(loss.item())\n            \n            if _%10 == 0:\n                wandb.log({\"Validation Loss\": loss.item()})\n    return losses","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T17:07:22.045066Z","iopub.execute_input":"2025-04-04T17:07:22.045374Z","iopub.status.idle":"2025-04-04T17:07:22.052014Z","shell.execute_reply.started":"2025-04-04T17:07:22.045346Z","shell.execute_reply":"2025-04-04T17:07:22.051097Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"def main(args):\n    \n    # train_df = get_df_from_gcs_blob(args.gcs_train_blob, bucket=args.gcs_bucket)\n    # test_df = get_df_from_gcs_blob(args.gcs_test_blob, bucket=args.gcs_bucket)\n\n    train_df = train_data\n    test_df = test_data\n    train_df = train_df[train_df['input'].map(str).map(len) < args.inp_max_len].reset_index(drop=True)\n    \n    TRAIN_BATCH_SIZE = args.train_batch_size\n    TEST_BATCH_SIZE = args.test_batch_size\n    TRAIN_NUM_WORKERS = args.train_num_workers\n    TEST_NUM_WORKERS = args.test_num_workers\n    MOD_SAVE_PATH = args.mod_save_path\n\n    INP_MAX_LEN = max(train_df['input'].map(len).max(), test_df['input'].map(len).max())\n    OUT_MAX_LEN = max(train_df['output'].map(len).max(), test_df['output'].map(len).max())\n\n    MOD = 't5-small'\n    EPOCHS = args.epochs\n    LR = args.lr\n    DEVICE = 'cuda' if cuda.is_available() else 'cpu'\n    print(f'Using device: {DEVICE}')\n    FP16 = bool(args.fp16)\n\n    # start a new wandb run to track this script\n    wandb.init(\n        # set the wandb project where this run will be logged\n        project=\"recipe-t5\",\n        \n        # track hyperparameters and run metadata\n        config={\n        \"epochs\": EPOCHS,\n        \"train_data_batch_size\": TRAIN_BATCH_SIZE,\n        \"train_dataloader_num_workers\": TRAIN_NUM_WORKERS,\n        \"test_data_batch_size\": TEST_BATCH_SIZE,\n        \"test_dataloader_num_workers\": TEST_NUM_WORKERS,\n        \"inp_max_len\": INP_MAX_LEN,\n        \"out_max_len\": OUT_MAX_LEN,\n        \"train_data_shape\": train_df.shape,\n        \"test_data_shape\": test_df.shape,\n        \"device\": DEVICE,\n        \"lr\": LR,\n        \"model\": MOD,\n        \"fp16_enabled\": FP16,\n        \"exp_name\": args.exp_name\n        }\n    )\n\n    tokenizer = T5Tokenizer.from_pretrained(MOD)\n    #tokenizer.add_special_tokens({'additional_special_tokens': ['<ingredients>', '<calories>', '<title>', '<directions>']})\n\n    train_dataset = T5Dataset(train_df['ingredients'].values, train_df['recipe'].values, tokenizer, INP_MAX_LEN, OUT_MAX_LEN)\n    test_dataset = T5Dataset(test_df['ingredients'].values, test_df['recipe'].values, tokenizer, INP_MAX_LEN, OUT_MAX_LEN)\n\n    train_loader = DataLoader(train_dataset, batch_size=TRAIN_BATCH_SIZE, num_workers=TRAIN_NUM_WORKERS, shuffle=True)\n    test_loader = DataLoader(test_dataset, batch_size=TEST_BATCH_SIZE, num_workers=TEST_NUM_WORKERS)\n\n    model = T5ForConditionalGeneration.from_pretrained(MOD).to(DEVICE)\n\n    opt = torch.optim.Adam(params=model.parameters(), lr=LR)        \n    \n    for epoch in range(EPOCHS):\n        print(f\"Beginning training in epoch {epoch}...\")\n        start = perf_counter()\n        train_losses = train(tokenizer, model, DEVICE, train_loader, opt, fp16=FP16)\n        end = perf_counter()\n        wandb.log({\"Epoch Training Time (sec)\": end - start})\n\n        start = perf_counter()\n        test_losses = test(tokenizer, model, DEVICE, test_loader, fp16=FP16)\n        end = perf_counter()\n        wandb.log({\"Epoch Testing Time (sec)\": end - start})\n\n        epoch_running_train_loss = sum(train_losses)\n        wandb.log({\"Epoch Running Training Loss\": epoch_running_train_loss})\n        print(f\"Epoch {epoch} Running Train Loss: {epoch_running_train_loss}\")\n        epoch_running_test_loss = sum(test_losses)\n        wandb.log({\"Epoch Running Testing Loss\": epoch_running_test_loss})\n        print(f\"Epoch {epoch} Running Test Loss: {epoch_running_test_loss}\")\n\n        if epoch % 100 == 0:\n            path = f'{MOD_SAVE_PATH}/{epoch}.pt'\n            save_model(path, epoch, model, opt, epoch_running_train_loss)\n\n    wandb.finish()\n    print('Saving model and tokenizer!')\n    model.save_pretrained(f'{MOD_SAVE_PATH}/final')\n    tokenizer.save_pretrained(f'{MOD_SAVE_PATH}/final')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T17:04:08.807896Z","iopub.execute_input":"2025-04-04T17:04:08.808140Z","iopub.status.idle":"2025-04-04T17:04:08.819526Z","shell.execute_reply.started":"2025-04-04T17:04:08.808088Z","shell.execute_reply":"2025-04-04T17:04:08.818579Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"model = T5ForConditionalGeneration.from_pretrained('t5-small')\ntokenizer = T5Tokenizer.from_pretrained('t5-small')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T17:04:48.674335Z","iopub.execute_input":"2025-04-04T17:04:48.674707Z","iopub.status.idle":"2025-04-04T17:04:53.005423Z","shell.execute_reply.started":"2025-04-04T17:04:48.674676Z","shell.execute_reply":"2025-04-04T17:04:53.004711Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"213a8028629d4bbd83748b7b3387122c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/242M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fb6acbca21554d9fab820a3ae69e3ae1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2b6bf8f56f4749b09da0c83c704b1a40"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/2.32k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1016251dbcf148528b83b6706841990b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8a5502be490a46ccbaa6735d7b7b81c9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.39M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"04d793e41ed844b8a07a45fab50fa7c9"}},"metadata":{}},{"name":"stderr","text":"You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"optimzer = torch.optim.Adam(params=model.parameters(), lr=1e-4)\n\ntrain(tokenizer, model, device, loader, optimizer)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}