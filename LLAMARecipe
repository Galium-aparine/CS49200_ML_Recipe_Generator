{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":1349767,"sourceType":"datasetVersion","datasetId":785504}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport transformers\nfrom transformers import T5Tokenizer, T5ForConditionalGeneration\nimport torch\nfrom datasets import load_dataset\nimport wandb\nimport numpy as np\nimport math\nimport random","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-04T16:09:48.404403Z","iopub.execute_input":"2025-04-04T16:09:48.404757Z","iopub.status.idle":"2025-04-04T16:09:48.409080Z","shell.execute_reply.started":"2025-04-04T16:09:48.404735Z","shell.execute_reply":"2025-04-04T16:09:48.408131Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"# import dataset\n# dataset obtained from \n# https://www.kaggle.com/datasets/snehallokesh31096/recipe\ndf = pd.read_csv('/kaggle/input/recipe/recipes_82k.csv')\n# dropping unnecessary columns\ndf.drop(['category'],axis=1,inplace=True)\n# concatenating the title and recipe\ndf['recipe']=df.apply(lambda x:'%s: %s' % (df['recipe_name'],df['cooking_method']),axis=1)\n# saving df as a CSV file\ndf.to_csv('/kaggle/working/recipe_cleaned.csv', index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T16:03:21.144828Z","iopub.execute_input":"2025-04-04T16:03:21.145162Z","iopub.status.idle":"2025-04-04T16:04:56.103327Z","shell.execute_reply.started":"2025-04-04T16:03:21.145139Z","shell.execute_reply":"2025-04-04T16:04:56.102496Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"# TODO: create a randomly generated list for train and test\n# by using random numbers within a given range and store in an array\nstart = 0\nend = len(df)-1\nindx = np.array([random.randint(start, end) for i in range(math.floor(len(df)*.80))])\n\n# loading df for train and test\ntrain_data = df.iloc[indx]\ntest_data = df.iloc[-indx]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T16:05:25.062072Z","iopub.execute_input":"2025-04-04T16:05:25.062361Z","iopub.status.idle":"2025-04-04T16:05:25.184457Z","shell.execute_reply.started":"2025-04-04T16:05:25.062340Z","shell.execute_reply":"2025-04-04T16:05:25.183172Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"# COPIED FROM https://www.kaggle.com/code/kreeshrajani/fine-tune-t5-for-conversational-model\nclass T5Dataset:\n  def __init__(self, inps, outs, tokenizer, inp_max_len, out_max_len):   \n    self.inps = inps\n    self.outs = outs\n    self.tokenizer = tokenizer\n    self.input_max_len = inp_max_len\n    self.output_max_len = out_max_len\n  \n  def __len__(self):                      # This method retrives the number of item from the dataset\n    return len(self.inps)\n\n  def __getitem__(self, item):             # This method retrieves the item at the specified index item. \n    inp = str(self.inps[item])\n    out = str(self.outs[item])\n\n    input_tokenize = self.tokenizer(      \n            inp,\n            add_special_tokens=True,\n            max_length=self.input_max_len,\n            padding = 'max_length',\n            truncation = True,\n            return_attention_mask=True,\n            return_tensors=\"pt\"\n        )\n    output_tokenize = self.tokenizer(\n            out,\n            add_special_tokens=True,\n            max_length=self.output_max_len,\n            padding = 'max_length',\n            truncation = True,\n            return_attention_mask=True,\n            return_tensors=\"pt\"\n            \n        )\n    \n\n    input_ids = input_tokenize[\"input_ids\"].flatten().to(dtype=torch.long)\n    attention_mask = input_tokenize[\"attention_mask\"].flatten().to(dtype=torch.long)\n    output_ids = output_tokenize['input_ids'].flatten().to(dtype=torch.long)\n\n    out = {\n            'input': inp,      \n            'target': out,\n            'input_ids': input_ids,\n            'attention_mask': attention_mask,\n            'target_ids': output_ids\n        }\n        \n    return out ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T16:05:29.071869Z","iopub.execute_input":"2025-04-04T16:05:29.072166Z","iopub.status.idle":"2025-04-04T16:05:29.079571Z","shell.execute_reply.started":"2025-04-04T16:05:29.072147Z","shell.execute_reply":"2025-04-04T16:05:29.078724Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"def train(tokenizer, model, device, loader, optimizer, fp16=True):\n    losses = []\n    if fp16: model.half()\n    model.train()\n    for _, data in enumerate(loader, 0):\n        y = train_data['recipe'].to(device, dtype = torch.long)\n        y_ids = y[:, :-1].contiguous()\n        lm_labels = y[:, 1:].clone().detach()\n        lm_labels[y[:, 1:] == tokenizer.pad_token_id] = -100\n        # ids = data['input_ids'].to(device, dtype = torch.long)\n        # mask = data['attention_mask'].to(device, dtype = torch.long)\n\n        # outputs = model(input_ids = ids, attention_mask = mask, decoder_input_ids=y_ids, labels=lm_labels)\n        outputs = model(decoder_input_ids=y_ids, labels=lm_labels)\n        loss = outputs[0]\n        losses.append(loss.item())\n        \n        if _%10 == 0:\n            wandb.log({\"Training Loss\": loss.item()})\n        \n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n    return losses     ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T16:05:33.341353Z","iopub.execute_input":"2025-04-04T16:05:33.341791Z","iopub.status.idle":"2025-04-04T16:05:33.349935Z","shell.execute_reply.started":"2025-04-04T16:05:33.341756Z","shell.execute_reply":"2025-04-04T16:05:33.348883Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"# test dataset\ndef test(tokenizer, model, device, loader, fp16=True):\n    losses = []\n    if fp16: model.half()\n    model.eval()\n    with torch.no_grad():\n        for _, data in enumerate(loader, 0):\n            y = test_data['recipe'].to(device, dtype = torch.long)\n            y_ids = y[:, :-1].contiguous()\n            lm_labels = y[:, 1:].clone().detach()\n            lm_labels[y[:, 1:] == tokenizer.pad_token_id] = -100\n            # ids = data['input_ids'].to(device, dtype = torch.long)\n            # mask = data['attention_mask'].to(device, dtype = torch.long)\n\n            # outputs = model(input_ids = ids, attention_mask = mask, decoder_input_ids=y_ids, labels=lm_labels)\n            outputs = model(decoder_input_ids=y_ids, labels=lm_labels)\n            loss = outputs[0]\n            losses.append(loss.item())\n            \n            if _%10 == 0:\n                wandb.log({\"Validation Loss\": loss.item()})\n    return losses","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T16:05:36.877278Z","iopub.execute_input":"2025-04-04T16:05:36.877562Z","iopub.status.idle":"2025-04-04T16:05:36.883309Z","shell.execute_reply.started":"2025-04-04T16:05:36.877541Z","shell.execute_reply":"2025-04-04T16:05:36.882571Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"def main(args):\n    \n    # train_df = get_df_from_gcs_blob(args.gcs_train_blob, bucket=args.gcs_bucket)\n    # test_df = get_df_from_gcs_blob(args.gcs_test_blob, bucket=args.gcs_bucket)\n\n    train_df = train_data\n    test_df = test_data\n    train_df = train_df[train_df['input'].map(str).map(len) < args.inp_max_len].reset_index(drop=True)\n    \n    TRAIN_BATCH_SIZE = args.train_batch_size\n    TEST_BATCH_SIZE = args.test_batch_size\n    TRAIN_NUM_WORKERS = args.train_num_workers\n    TEST_NUM_WORKERS = args.test_num_workers\n    MOD_SAVE_PATH = args.mod_save_path\n\n    INP_MAX_LEN = max(train_df['input'].map(len).max(), test_df['input'].map(len).max())\n    OUT_MAX_LEN = max(train_df['output'].map(len).max(), test_df['output'].map(len).max())\n\n    MOD = 't5-small'\n    EPOCHS = args.epochs\n    LR = args.lr\n    DEVICE = 'cuda' if cuda.is_available() else 'cpu'\n    print(f'Using device: {DEVICE}')\n    FP16 = bool(args.fp16)\n\n    # start a new wandb run to track this script\n    wandb.init(\n        # set the wandb project where this run will be logged\n        project=\"recipe-t5\",\n        \n        # track hyperparameters and run metadata\n        config={\n        \"epochs\": EPOCHS,\n        \"train_data_batch_size\": TRAIN_BATCH_SIZE,\n        \"train_dataloader_num_workers\": TRAIN_NUM_WORKERS,\n        \"test_data_batch_size\": TEST_BATCH_SIZE,\n        \"test_dataloader_num_workers\": TEST_NUM_WORKERS,\n        \"inp_max_len\": INP_MAX_LEN,\n        \"out_max_len\": OUT_MAX_LEN,\n        \"train_data_shape\": train_df.shape,\n        \"test_data_shape\": test_df.shape,\n        \"device\": DEVICE,\n        \"lr\": LR,\n        \"model\": MOD,\n        \"fp16_enabled\": FP16,\n        \"exp_name\": args.exp_name\n        }\n    )\n\n    tokenizer = T5Tokenizer.from_pretrained(MOD)\n    #tokenizer.add_special_tokens({'additional_special_tokens': ['<ingredients>', '<calories>', '<title>', '<directions>']})\n\n    train_dataset = T5Dataset(train_df['ingredients'].values, train_df['recipe'].values, tokenizer, INP_MAX_LEN, OUT_MAX_LEN)\n    test_dataset = T5Dataset(test_df['ingredients'].values, test_df['recipe'].values, tokenizer, INP_MAX_LEN, OUT_MAX_LEN)\n\n    train_loader = DataLoader(train_dataset, batch_size=TRAIN_BATCH_SIZE, num_workers=TRAIN_NUM_WORKERS, shuffle=True)\n    test_loader = DataLoader(test_dataset, batch_size=TEST_BATCH_SIZE, num_workers=TEST_NUM_WORKERS)\n\n    model = T5ForConditionalGeneration.from_pretrained(MOD).to(DEVICE)\n\n    opt = torch.optim.Adam(params=model.parameters(), lr=LR)        \n    \n    for epoch in range(EPOCHS):\n        print(f\"Beginning training in epoch {epoch}...\")\n        start = perf_counter()\n        train_losses = train(tokenizer, model, DEVICE, train_loader, opt, fp16=FP16)\n        end = perf_counter()\n        wandb.log({\"Epoch Training Time (sec)\": end - start})\n\n        start = perf_counter()\n        test_losses = test(tokenizer, model, DEVICE, test_loader, fp16=FP16)\n        end = perf_counter()\n        wandb.log({\"Epoch Testing Time (sec)\": end - start})\n\n        epoch_running_train_loss = sum(train_losses)\n        wandb.log({\"Epoch Running Training Loss\": epoch_running_train_loss})\n        print(f\"Epoch {epoch} Running Train Loss: {epoch_running_train_loss}\")\n        epoch_running_test_loss = sum(test_losses)\n        wandb.log({\"Epoch Running Testing Loss\": epoch_running_test_loss})\n        print(f\"Epoch {epoch} Running Test Loss: {epoch_running_test_loss}\")\n\n        if epoch % 100 == 0:\n            path = f'{MOD_SAVE_PATH}/{epoch}.pt'\n            save_model(path, epoch, model, opt, epoch_running_train_loss)\n\n    wandb.finish()\n    print('Saving model and tokenizer!')\n    model.save_pretrained(f'{MOD_SAVE_PATH}/final')\n    tokenizer.save_pretrained(f'{MOD_SAVE_PATH}/final')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T16:09:52.814502Z","iopub.execute_input":"2025-04-04T16:09:52.814887Z","iopub.status.idle":"2025-04-04T16:09:52.825608Z","shell.execute_reply.started":"2025-04-04T16:09:52.814859Z","shell.execute_reply":"2025-04-04T16:09:52.824864Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"DEVICE = 'cuda'\nMOD_PATH = './inp_cal_ingred_cal/final'\n\nmodel = T5ForConditionalGeneration.from_pretrained(MOD_PATH).to(DEVICE)\ntokenizer = T5Tokenizer.from_pretrained(MOD_PATH)\n\noob_model = T5ForConditionalGeneration.from_pretrained('t5-small').to(DEVICE)\noob_tokenizer = T5Tokenizer.from_pretrained('t5-small')","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}